#!/usr/bin/env python3
# .claude/scripts/analyze_log.py - NDJSON ãƒ­ã‚°è§£æžã‚¹ã‚¯ãƒªãƒ—ãƒˆ
#
# ä½¿ã„æ–¹:
#   python analyze_log.py                    # æœ€æ–°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’è§£æž
#   python analyze_log.py --category deps    # ç‰¹å®šã‚«ãƒ†ã‚´ãƒªã®ã¿
#   python analyze_log.py --level error      # ã‚¨ãƒ©ãƒ¼ã®ã¿
#   python analyze_log.py --json             # JSON å½¢å¼ã§å‡ºåŠ›
#   python analyze_log.py --stats            # çµ±è¨ˆæƒ…å ±ã®ã¿
#   python analyze_log.py --list             # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸€è¦§

from __future__ import annotations

import argparse
import json
import sys
from collections import Counter
from collections.abc import Iterator
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any


# ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
SCRIPT_DIR = Path(__file__).parent
LOG_DIR = SCRIPT_DIR.parent / "logs"


@dataclass
class LogEntry:
    """ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒª"""

    session_id: str
    timestamp: int
    level: str
    category: str
    message: str
    data: dict[str, Any]

    @classmethod
    def from_dict(cls, d: dict[str, Any]) -> LogEntry:
        return cls(
            session_id=d.get("session_id", ""),
            timestamp=d.get("timestamp", 0),
            level=d.get("level", "info"),
            category=d.get("category", "general"),
            message=d.get("message", ""),
            data={
                k: v
                for k, v in d.items()
                if k not in ("session_id", "timestamp", "level", "category", "message")
            },
        )

    @property
    def datetime(self) -> datetime:
        return datetime.fromtimestamp(self.timestamp / 1000)

    def format_human(self) -> str:
        """äººé–“å¯èª­å½¢å¼"""
        time_str = self.datetime.strftime("%H:%M:%S.%f")[:-3]
        level_icon = {"debug": "ðŸ”", "info": "â„¹ï¸", "warn": "âš ï¸", "error": "âŒ"}.get(self.level, "â€¢")
        data_str = ""
        if self.data:
            data_str = " " + " ".join(f"{k}={v}" for k, v in self.data.items())
        return f"{time_str} {level_icon} [{self.category}] {self.message}{data_str}"


def find_latest_log() -> Path | None:
    """æœ€æ–°ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŽ¢ã™"""
    # latest.ndjson ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã‚’ç¢ºèª
    latest_link = LOG_DIR / "latest.ndjson"
    if latest_link.exists():
        if latest_link.is_symlink():
            return latest_link.resolve()
        return latest_link

    # latest.txtï¼ˆã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ä»£æ›¿ï¼‰ã‚’ç¢ºèª
    latest_txt = LOG_DIR / "latest.txt"
    if latest_txt.exists():
        target_name = latest_txt.read_text().strip()
        target_path = LOG_DIR / target_name
        if target_path.exists():
            return target_path

    # ãªã‘ã‚Œã°æœ€æ–°ã® .ndjson ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŽ¢ã™
    log_files = sorted(
        [f for f in LOG_DIR.glob("*.ndjson") if f.name != "latest.ndjson"],
        key=lambda f: f.stat().st_mtime,
        reverse=True,
    )
    return log_files[0] if log_files else None


def list_sessions() -> list[dict[str, Any]]:
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸€è¦§ã‚’å–å¾—"""
    sessions = []
    for log_file in sorted(LOG_DIR.glob("*.ndjson"), reverse=True):
        if log_file.name == "latest.ndjson":
            continue
        try:
            size = log_file.stat().st_size
            with log_file.open(encoding="utf-8") as fp:
                line_count = sum(1 for _ in fp)
            sessions.append(
                {
                    "file": log_file.name,
                    "size": size,
                    "entries": line_count,
                    "modified": datetime.fromtimestamp(log_file.stat().st_mtime),
                },
            )
        except Exception:
            pass
    return sessions


def parse_log(path: Path) -> Iterator[LogEntry]:
    """NDJSON ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‘ãƒ¼ã‚¹"""
    with path.open(encoding="utf-8") as fp:
        for line in fp:
            line = line.strip()
            if not line:
                continue
            try:
                data = json.loads(line)
                yield LogEntry.from_dict(data)
            except json.JSONDecodeError:
                continue


def filter_entries(
    entries: Iterator[LogEntry],
    category: str | None = None,
    level: str | None = None,
    search: str | None = None,
) -> Iterator[LogEntry]:
    """ã‚¨ãƒ³ãƒˆãƒªã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    level_order = {"debug": 0, "info": 1, "warn": 2, "error": 3}
    min_level = level_order.get(level, 0) if level else 0

    for entry in entries:
        if category and entry.category != category:
            continue
        if level_order.get(entry.level, 0) < min_level:
            continue
        if search and search.lower() not in entry.message.lower():
            continue
        yield entry


def compute_stats(entries: list[LogEntry]) -> dict[str, Any]:
    """çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—"""
    if not entries:
        return {"total": 0}

    by_level = Counter(e.level for e in entries)
    by_category = Counter(e.category for e in entries)

    # æ™‚é–“ç¯„å›²
    timestamps = [e.timestamp for e in entries]
    duration_ms = max(timestamps) - min(timestamps)

    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãƒ­ã‚°ã®é›†è¨ˆ
    profile_entries = [e for e in entries if e.category == "profile"]
    slow_operations = []
    if profile_entries:
        for e in profile_entries:
            if "duration_ms" in e.data and e.data["duration_ms"] > 10:
                slow_operations.append(
                    {
                        "scope": e.data.get("scope", "unknown"),
                        "duration_ms": e.data["duration_ms"],
                    },
                )
        slow_operations.sort(key=lambda x: x["duration_ms"], reverse=True)

    # ä¾å­˜é–¢ä¿‚é•å
    violations = []
    for e in entries:
        if e.category == "deps" and e.data.get("violations"):
            violations.extend(e.data["violations"])

    return {
        "total": len(entries),
        "by_level": dict(by_level),
        "by_category": dict(by_category),
        "duration_ms": duration_ms,
        "slow_operations": slow_operations[:10],  # Top 10
        "violations": list(set(violations)),
    }


def print_human(entries: list[LogEntry], stats: dict[str, Any]) -> None:
    """äººé–“å¯èª­å½¢å¼ã§å‡ºåŠ›"""
    # ãƒ˜ãƒƒãƒ€ãƒ¼
    print(f"\n{'='*60}")
    print(f"ðŸ“‹ Log Analysis - {stats['total']} entries")
    print(f"{'='*60}\n")

    # çµ±è¨ˆã‚µãƒžãƒªãƒ¼
    if stats["total"] > 0:
        print("ðŸ“Š Summary:")
        print(f"   Levels:     {stats['by_level']}")
        print(f"   Categories: {stats['by_category']}")
        if stats["duration_ms"] > 0:
            print(f"   Duration:   {stats['duration_ms']:.0f}ms")
        print()

    # é•åãŒã‚ã‚Œã°è¡¨ç¤º
    if stats.get("violations"):
        print("âš ï¸ Layer Violations:")
        for v in stats["violations"]:
            print(f"   - {v}")
        print()

    # é…ã„å‡¦ç†ãŒã‚ã‚Œã°è¡¨ç¤º
    if stats.get("slow_operations"):
        print("ðŸ¢ Slow Operations (>10ms):")
        for op in stats["slow_operations"][:5]:
            print(f"   - {op['scope']}: {op['duration_ms']:.1f}ms")
        print()

    # ã‚¨ãƒ³ãƒˆãƒªä¸€è¦§
    print("ðŸ“ Entries:")
    for entry in entries:
        print(f"   {entry.format_human()}")


def print_json(entries: list[LogEntry], stats: dict[str, Any]) -> None:
    """JSON å½¢å¼ã§å‡ºåŠ›"""
    output = {
        "stats": stats,
        "entries": [
            {
                "timestamp": e.timestamp,
                "level": e.level,
                "category": e.category,
                "message": e.message,
                **e.data,
            }
            for e in entries
        ],
    }
    print(json.dumps(output, indent=2, ensure_ascii=False, default=str))


def main():
    parser = argparse.ArgumentParser(description="PME mini ãƒ­ã‚°è§£æžãƒ„ãƒ¼ãƒ«")
    parser.add_argument("file", nargs="?", help="è§£æžã™ã‚‹ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆçœç•¥æ™‚ã¯ latestï¼‰")
    parser.add_argument("-c", "--category", help="ã‚«ãƒ†ã‚´ãƒªã§ãƒ•ã‚£ãƒ«ã‚¿")
    parser.add_argument(
        "-l",
        "--level",
        choices=["debug", "info", "warn", "error"],
        help="æœ€å°ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«",
    )
    parser.add_argument("-s", "--search", help="ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ¤œç´¢")
    parser.add_argument("--json", action="store_true", help="JSON å½¢å¼ã§å‡ºåŠ›")
    parser.add_argument("--stats", action="store_true", help="çµ±è¨ˆæƒ…å ±ã®ã¿")
    parser.add_argument("--list", action="store_true", help="ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸€è¦§")
    parser.add_argument("--tail", type=int, default=0, help="æœ€æ–° N ä»¶ã®ã¿")

    args = parser.parse_args()

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸€è¦§
    if args.list:
        sessions = list_sessions()
        if not sessions:
            print("No log sessions found.")
            return
        print(f"\n{'='*60}")
        print("ðŸ“ Log Sessions")
        print(f"{'='*60}\n")
        for s in sessions:
            print(f"   {s['file']}")
            print(f"      Entries: {s['entries']:,}  Size: {s['size']:,} bytes")
            print(f"      Modified: {s['modified']}")
            print()
        return

    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«æ±ºå®š
    if args.file:
        log_path = Path(args.file)
        if not log_path.exists():
            log_path = LOG_DIR / args.file
    else:
        log_path = find_latest_log()

    if not log_path or not log_path.exists():
        print("No log file found. Run PME mini first to generate logs.")
        sys.exit(1)

    print(f"Analyzing: {log_path.name}")

    # ãƒ‘ãƒ¼ã‚¹ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    entries = list(
        filter_entries(
            parse_log(log_path),
            category=args.category,
            level=args.level,
            search=args.search,
        ),
    )

    # tail ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    if args.tail > 0:
        entries = entries[-args.tail :]

    # çµ±è¨ˆè¨ˆç®—
    stats = compute_stats(entries)

    # å‡ºåŠ›
    if args.json:
        print_json(entries, stats)
    elif args.stats:
        print(json.dumps(stats, indent=2, ensure_ascii=False, default=str))
    else:
        print_human(entries, stats)


if __name__ == "__main__":
    main()
